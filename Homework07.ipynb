{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QshK8s21WBrf"
   },
   "source": [
    "# Homework07\n",
    "\n",
    "Exercises to practice pandas, data analysis and classification\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Understand the effects of pre-processing data\n",
    "- Get familiar with the ML flow: encode -> normalize -> train -> evaluate\n",
    "- Understand the difference between regression and classification tasks\n",
    "- Build an intuition for different classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Hf8SXUwWOho"
   },
   "source": [
    "### Setup\n",
    "\n",
    "Run the following 2 cells to import all necessary libraries and helpers for this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/PSAM-5020-2025S-A/5020-utils/raw/main/src/data_utils.py\n",
    "!wget -q https://github.com/PSAM-5020-2025S-A/5020-utils/raw/main/src/image_utils.py\n",
    "\n",
    "!wget -qO- https://github.com/PSAM-5020-2025S-A/5020-utils/releases/latest/download/0801-500.tar.gz | tar xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import PIL.Image as PImage\n",
    "\n",
    "from os import listdir, path\n",
    "\n",
    "from data_utils import RandomForestClassifier, SVC\n",
    "from data_utils import classification_error, display_confusion_matrix, regression_error\n",
    "\n",
    "from image_utils import get_pixels, make_image\n",
    "\n",
    "from Homework07_utils import CamUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "The dataset we are going to use has images from $25$ different security cameras, and our task is to separate them by camera. Some of the cameras move, some of them don't, and there are more than $1000$ images, so there's no way we want to do this by hand.\n",
    "\n",
    "### Loading Data\n",
    "\n",
    "If we look at the images in `./data/image/0801-500/train/`, we'll notice that they are named and organized in a very particular way. They're all in the same directory and the first part of their filename specifies which camera they came from. Even though those `ids` are numbers, they're not sequential, so we'll use some helper functions to extract a unique `label` from their filenames.\n",
    "\n",
    "This is exactly what the `OrdinalEncoder` class does, but since we only have to encode this one column, we'll do it by hand while we read the files in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this creates a list of all the files in a given directory, that end in .jpg\n",
    "train_files = [f for f in listdir(\"./data/image/0801-500/train\") if f.endswith(\".jpg\")] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check and see what is inside the list here\n",
    "train_files[:10]\n",
    "# print(len(train_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll read the image pixels and extract their labels. `CamUtils.get_label()` is the helper function we'll use to \"encode\" and return a label id based on the filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_data = []\n",
    "label_data = []\n",
    "\n",
    "#create a loop that reads the pixel data and label_data \n",
    "for fname in train_files:\n",
    "  label = CamUtils.get_label(fname)\n",
    "  img = PImage.open(path.join(\"./data/image/0801-500/train\", fname))\n",
    "  label_data.append(label)\n",
    "  pixel_data.append(list(img.getdata()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: check if labels got extracted correctly by looking at \n",
    "# the first few items of the label list and the filename list\n",
    "print(label_data[:6])\n",
    "print(pixel_data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels and the filenames won't match exactly since labels start at $0$ and the filenames start at $01$ and skip some numbers.\n",
    "\n",
    "We can open some images from pixels, just to make sure we loaded them correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(make_image(pixel_data[200]))\n",
    "display(make_image(pixel_data[100]))\n",
    "display(make_image(pixel_data[150]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now might not be a bad time to peek into the `data/image/0801-500/` directories to see what's inside them and what the images look like.... and get to know the data..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame it\n",
    "\n",
    "Let's put our raw pixel data into a `DataFrame`, and create a column for storing each image's label.\n",
    "\n",
    "(this next cell might take a while to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df = pd.DataFrame(pixel_data)\n",
    "train_df[\"lable\"] = label_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect our `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.shape)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insight\n",
    "\n",
    "<span style=\"color:hotpink\">\n",
    "Does anything stand out as peculiar about the feature values in our <code>DataFrame</code>?<br>\n",
    "Do we have to encode or scale our data?<br>\n",
    "Why? Or, why not?<br>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "source": [
    "\n",
    "<span style=\"color:lightblue;\"> The only thing that I'm seeing is that all the images have the same number of pixels.<br>\n",
    "Also they are black and white meaning each feather is a point on the gray scale. <br>\n",
    "I think we do not need scaling as the features are on the same scale from 0 to 255.<br>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Files\n",
    "\n",
    "If that worked, repeat the process for the test files inside the `./data/image/0801-500/test/` directory.\n",
    "\n",
    "We can almost use the exact same steps as we did above to create a `DataFrame`, the only difference being that we don't have labels for these images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: create a list of files in the test/ directory\n",
    "TEST_PATH = \"/workspaces/Homework07/data/image/0801-500/test\"\n",
    "test_files = [f for f in listdir(TEST_PATH) if f.endswith(\".jpg\")]\n",
    "\n",
    "# TODO: check its length and content\n",
    "len(test_files)\n",
    "\n",
    "test_pixel_data = []\n",
    "\n",
    "# TODO: loop over files and load their pixels into a list\n",
    "for fname in test_files:\n",
    "    img = PImage.open(path.join(TEST_PATH, fname))\n",
    "    test_pixel_data.append(list(img.getdata()))\n",
    "\n",
    "# TODO: load into DataFrame (this might take 20 - 30 seconds)\n",
    "test_df = pd.DataFrame(test_pixel_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like data!!\n",
    "\n",
    "We could train a `RandomForestClassifier` directly on this `DataFrame` and see what would happen, but my guess is that Python runs out of memory and crashes our tab/browser/computer...\n",
    "\n",
    "We'll use _projection_ to reduce the number of dimensions in our dataset. Projection is when we just drop some of the columns in our dataset. \n",
    "\n",
    "Which columns ? That's up to us.\n",
    "\n",
    "Let's first try using the first $N$ columns/features where $N$ is a number around $10$.\n",
    "\n",
    "This is how we get the first $N$ columns from a `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split input and output features\n",
    "NUM_FEATURES = 10\n",
    "chosen_columns = train_df.columns[:NUM_FEATURES]\n",
    "train_features = train_df[chosen_columns]\n",
    "\n",
    "out_features = train_df[\"lable\"]\n",
    "\n",
    "# also separate test dataset features\n",
    "test_features = test_df[chosen_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our [Week 07](https://github.com/PSAM-5020-2025S-A/WK07) notebook, we can create a classification model by following these steps:\n",
    "\n",
    "1. Load dataset (done! üéâ)\n",
    "2. Encode label features as numbers (not needed! done! ‚ö°Ô∏è)\n",
    "3. Normalize the data (not needed! done! üçæ)\n",
    "4. Separate the outcome variable and the input features (done! ‚òÄÔ∏è)\n",
    "5. Create a model using chosen features\n",
    "6. Run model on training data and measure error*\n",
    "7. Run model on test data, measure error*, plot predictions, interpret results\n",
    "\n",
    "We could use the same `regression_error()` function we used above to measure the error of our classifier model, but this could lead to $2$ issues. First, we don't have labels for the images in the test dataset, and second, the regression error reported might be higher than it actually is because an image with label $0$ that gets mislabeled as $5$ will count as being more wrong than if it was mislabeled $2$. And we don't want that. We just want to get the percentage of classifications that our model gets correctly.\n",
    "\n",
    "To simplify calculating the classification accuracy we can use the `CamUtils.classification_accuracy()` function. This function takes $2$ parameters, a list of files and a list of predictions. It will work with the test and train datasets and will calculate a more meaningful accuracy value than the one returned by `regression_error()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: create a brand new classifier\n",
    "from data_utils import SGDClassifier\n",
    "#First I wanted to try SGDClassifier but the accuracy result was very low at 0.21 level.\n",
    "#So I went with Random Forest first to check the results. \n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# TODO: fit the model\n",
    "model.fit(train_features, out_features)\n",
    "\n",
    "# TODO: run predictions\n",
    "train_predictions = model.predict(train_features)\n",
    "\n",
    "# TODO: measure classification accuracy\n",
    "CamUtils.classification_accuracy(train_files, train_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should look promising. Let's run this on our test dataset.\n",
    "\n",
    "Remember we already separated the test data features into a variable called `test_features` above.\n",
    "\n",
    "Now we just have to run the prediction and measure accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: run predictions on test data\n",
    "test_predictions = model.predict(test_features)\n",
    "\n",
    "# TODO: measure classification accuracy\n",
    "CamUtils.classification_accuracy(test_files, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "Using just the first $10$ pixels of the image the classifier is able to label most of the images correctly.\n",
    "\n",
    "<span style=\"color:hotpink\">\n",
    "How can we improve this classifier? How does the number of features affect the classification accuracy of the test data<br>\n",
    "How does the choice of pixels affect the accuracy?<br><br>\n",
    "If you're curious, repeat the modeling above, but using the <code>SVC</code> classifier instead of <code>RandomForest</code>.<br>How does the choice of modeling technique affect the accuracy?<br><br>\n",
    "Experiment with some of these parameters and explain your findings below.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "source": [
    "<span style=\"color:lightgreen;\">\n",
    "To improve the classifier I would think about increasing the number of pixels (features) and have more data to train the model with. <br>\n",
    "In the nest cells I will use another classifier and see how the results change.\n",
    "</span> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying SVC Classifier \n",
    "from sklearn.svm import SVC as SVClassifier\n",
    "#The defult kernel is rbf as of ths scikit learning. So I just changed to see the imacpt. \n",
    "# svc_model = SVClassifier()\n",
    "# svc_model = SVClassifier(kernel=\"poly\", degree=3)\n",
    "# svc_model = SVClassifier(kernel=\"linear\", degree=3)\n",
    "\n",
    "svc_model = SVClassifier(kernel=\"linear\", degree=4)\n",
    "svc_model.fit(train_features, out_features)\n",
    "svc_predictions = svc_model.predict(train_features)\n",
    "CamUtils.classification_accuracy(train_files, svc_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:lightgreen\">\n",
    "By default and Support Vector Classification had 0.444 accuracy. <br>\n",
    "Changing the kernel to \"ploy\" has increased the accuracy to 0.512 <br>\n",
    "And with the linear kernel the result was 0.84 which is much better than the previous models. <br>\n",
    "I will use this on the test files and check the results. \n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_test_predictions = svc_model.predict(test_features)\n",
    "CamUtils.classification_accuracy(test_files, svc_test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:lightgreen\">\n",
    "The model performed .01 worst with SVC classifier <br>\n",
    "Next I will increase the number of featuers. \n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New Features \n",
    "NEW_NUM_FEATURES = 20\n",
    "NEW_chosen_columns = train_df.columns[:NEW_NUM_FEATURES]\n",
    "NEW_train_features = train_df[NEW_chosen_columns]\n",
    "NEW_test_features = test_df[NEW_chosen_columns]\n",
    "\n",
    "#Train new Model\n",
    "new_svc_model = SVClassifier(kernel=\"linear\")\n",
    "new_svc_model.fit(NEW_train_features, out_features)\n",
    "new_svc_predictions = new_svc_model.predict(NEW_train_features)\n",
    "print(\"Train Accuracy: \",CamUtils.classification_accuracy(train_files, new_svc_predictions))\n",
    "\n",
    "#Test new Model\n",
    "new_svc_test_predictions = new_svc_model.predict(NEW_test_features)\n",
    "print(\"Test Accuracy: \",CamUtils.classification_accuracy(test_files, new_svc_test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:lightgreen\"> \n",
    "The model did improve when increased the features but still not as good as Random Forest. <br>\n",
    "Let's check Random Forest classifier with the new 10 features\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train new Model\n",
    "new_rfc_model = RandomForestClassifier()\n",
    "new_rfc_model.fit(NEW_train_features, out_features)\n",
    "new_rfc_predictions = new_rfc_model.predict(NEW_train_features)\n",
    "print(\"Train Accuracy: \",CamUtils.classification_accuracy(train_files, new_svc_predictions))\n",
    "\n",
    "#Test new Model\n",
    "new_svc_test_predictions = new_rfc_model.predict(NEW_test_features)\n",
    "print(\"Test Accuracy: \", CamUtils.classification_accuracy(test_files, new_svc_test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:lightgreen\">\n",
    "The model is performing better with more features. <br>\n",
    "</span>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPxe2qYxIG7EblrvD1C4Pmv",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
